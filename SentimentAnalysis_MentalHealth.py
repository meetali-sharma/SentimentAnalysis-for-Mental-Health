# -*- coding: utf-8 -*-
"""SentimentAnalysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bl187j6lqkvshE2hw8KLzxFda1hsw4Fe
"""

# Load the dataset for Sentiment Analysis for mental health

import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, classification_report
from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV
from sklearn.preprocessing import label_binarize, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.multiclass import OneVsRestClassifier


Sentiment_data = pd.read_csv("Combined Data.csv")
Sentiment_data.head()

# Describe function gives us statistical terms of numeric columns.

print(Sentiment_data.describe())
Sentiment_data.shape              # shape attribute returns the number of rows and columns as a tuple.

# Check for missing or nan values
missing_values = Sentiment_data.isnull().sum()
missing_values

Sentiment_data.dropna(inplace=True)  # Remove rows with missing values.

# After handling the missing values, verify again to ensure no missing values remain.
missing_values = Sentiment_data.isnull().sum()
missing_values

print(Sentiment_data.columns)

Sentiment_data = Sentiment_data.drop_duplicates()     # drop duplicate values if any
Sentiment_data.shape
Sentiment_data = Sentiment_data.drop(columns=['Unnamed: 0'],axis=1)   # Remove the column Unnamed as it is unnecessary.
print(Sentiment_data.columns)                                         # Verify the column names after dropping the irrelevant column.

import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')

# Define the list of stopwords
stop_words = set(stopwords.words('english'))

# Apply the cleaning function to the 'statement' column, but handle NaN values
def clean_text(text):
    # Check if text is a string before cleaning
    if isinstance(text, str):
        # Remove special characters using regex
        text = re.sub(r"[^a-zA-Z\s]", "", text)

        # Convert text to lowercase
        text = text.lower()

        # Tokenize and remove stop words
        words = text.split()
        words = [word for word in words if word not in stop_words]

        # Join words back into a single string
        return " ".join(words)
    else:
        # Return an empty string if text is not a string (e.g., NaN)
        return ""

Sentiment_data['statement'] = Sentiment_data['statement'].apply(clean_text)
# Display the first few rows of the updated dataset
Sentiment_data.head()

Sentiment_data['status'].value_counts()

# Plot class distribution for the target variable
plt.figure(figsize=(10, 5))
sns.countplot(data=Sentiment_data, x='status')
plt.title('Class Distribution of status')
plt.xlabel('status')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

Sentiment_data.shape

pip install imbalanced-learn

from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV
from sklearn.metrics import classification_report, roc_auc_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.utils.class_weight import compute_class_weight

from imblearn.over_sampling import SMOTE
# Split dataset into train and validation sets
X = Sentiment_data['statement']  # Features
y = Sentiment_data['status'] # Target

# Perform stratified split (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Fit the vectorizer to the training data and transform it
X_train_vec = vectorizer.fit_transform(X_train)

# Transform the test data using the fitted vectorizer
X_test_vec = vectorizer.transform(X_test)

# Apply SMOTE to balance the classes in the training set
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_vec, y_train)

# Set up Stratified K-Folds Cross Validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

print("Training Data Shape:", X_train_resampled.shape)
print("Test Data Shape:", X_test_vec.shape) # Use X_test_vec here

# Stratified K-Folds with more splits
cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# ------------ Logistic Regression with Class Weights ------------

# Compute class weights
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train_resampled),
    y=y_train_resampled
)
class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}

# Pipeline for Logistic Regression
logreg_pipeline = Pipeline([
    ('tfidf', TfidfVectorizer()),  # TF-IDF Vectorizer
    ('logreg', LogisticRegression(class_weight=class_weight_dict, max_iter=1000, random_state=42))
])

# Cross-validate Logistic Regression
logreg_scores = []
for train_idx, val_idx in cv.split(X_train, y_train):
    X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]
    y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]

  # Convert the target variable to numerical labels using LabelEncoder
    label_encoder = LabelEncoder()
    y_train_fold_encoded = label_encoder.fit_transform(y_train_fold) # Fit and transform on the training fold
    y_val_fold_encoded = label_encoder.transform(y_val_fold) # Transform the validation fold

    logreg_pipeline.fit(X_train_fold, y_train_fold_encoded) # Use encoded labels for fitting
    y_val_pred = logreg_pipeline.predict(X_val_fold)
    y_val_proba = logreg_pipeline.predict_proba(X_val_fold)
    # Convert y_val_fold to numerical labels using the same LabelEncoder
    y_val_fold_encoded = label_encoder.transform(y_val_fold)
    logreg_scores.append(roc_auc_score(y_val_fold_encoded, y_val_proba, average='weighted', multi_class='ovr'))

print(f"Logistic Regression Average ROC-AUC (10-Fold CV): {np.mean(logreg_scores):.2f}")

# Final evaluation on the test set - Use LabelEncoder here as well
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)  # Fit on the entire y_train
y_test_encoded = label_encoder.transform(y_test)     # Transform y_test using the same encoder

logreg_pipeline.fit(X_train, y_train_encoded)
y_test_pred = logreg_pipeline.predict(X_test)
y_test_proba = logreg_pipeline.predict_proba(X_test)

print("Logistic Regression Test ROC-AUC:", roc_auc_score(y_test_encoded, y_test_proba, average='weighted', multi_class='ovr')) # Use encoded y_test
print("Logistic Regression Classification Report:\n", classification_report(y_test_encoded, y_test_pred)) # Use encoded y_test and y_test_pred

# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=3, random_state=42)
clf.fit(X_train_resampled, y_train_resampled)

# Make predictions
y_pred = clf.predict(X_test_vec)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')

y_pred_proba = clf.predict_proba(X_test_vec)


# Classification report
accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_proba, average='weighted', multi_class='ovr')
report = classification_report(y_test, y_pred)

# Print and plot the metrics
print(f"Accuracy: {accuracy:.2f}")
print(f"AUC: {roc_auc:.2f}")
print("Classification Report:")
print(report)

# Binarize the target variable for multi-class ROC
y_test_bin = label_binarize(y_test, classes=Sentiment_data['status'].unique()) # Assuming y_test is your original multi-class target
n_classes = y_test_bin.shape[1]

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])
    roc_auc[i] = roc_auc_score(y_test_bin[:, i], y_pred_proba[:, i])

# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(y_test_bin.ravel(), y_pred_proba.ravel())
roc_auc["micro"] = roc_auc_score(y_test_bin, y_pred_proba, average="micro", multi_class="ovr")

# Plot ROC curve for a specific class (e.g., class 0)
plt.figure()
plt.plot(fpr[0], tpr[0], label='ROC curve (area = %0.2f)' % roc_auc[0])
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic example')
plt.legend(loc="lower right")
plt.show()